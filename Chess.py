import chess
import chess.pgn
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import numba
import os
import sys
import csv
import subprocess
import time
import math

# Import HRM model and related functions
from hrm_model import HRMChess, PolicyValueDataset, train_step, train_loop, quick_train_eval

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def detect_gpu_memory_and_optimize_training():
    """
    GPU mem√≥ria detekt√°l√°s √©s automatikus batch size/learning rate optimaliz√°l√°s
    
    Returns:
        dict: Optimaliz√°lt training param√©terek
    """
    print(f"\nüîç GPU MEMORY DETECTION & TRAINING OPTIMIZATION")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available - GPU required for training!")
        print("üö® This training requires GPU acceleration.")
        print("üí° Please ensure CUDA is installed and GPU is available.")
        exit(1)
    
    try:
        # GPU inform√°ci√≥k lek√©rdez√©se
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        
        # Mem√≥ria inform√°ci√≥k
        total_memory = torch.cuda.get_device_properties(current_device).total_memory
        reserved_memory = torch.cuda.memory_reserved(current_device)
        allocated_memory = torch.cuda.memory_allocated(current_device)
        
        # El√©rhet≈ë mem√≥ria kisz√°m√≠t√°sa (GB-ban)
        total_gb = total_memory / (1024**3)
        available_gb = (total_memory - reserved_memory) / (1024**3)
        free_gb = (total_memory - allocated_memory) / (1024**3)
        
        print(f"üñ•Ô∏è GPU Device: {gpu_name}")
        print(f"üìä Total VRAM: {total_gb:.1f} GB")
        print(f"üìä Available VRAM: {available_gb:.1f} GB")
        print(f"üìä Free VRAM: {free_gb:.1f} GB")
        print(f"üî¢ CUDA Devices: {gpu_count}")
        
        # Batch size optimaliz√°l√°s SZABAD GPU mem√≥ria alapj√°n
        if free_gb >= 20:  # B≈ëven van szabad hely high-end k√°rty√°kon
            batch_config = {
                'batch_size': 64,
                'lr_multiplier': 1.5,  # Nagyobb batch ‚Üí nagyobb LR
                'optimization_level': 'HIGH_END_FREE'
            }
            print(f"üöÄ HIGH-END FREE MEMORY ({free_gb:.1f}GB+ available)")
            
        elif free_gb >= 14:  # J√≥ mennyis√©g≈± szabad mem√≥ria
            batch_config = {
                'batch_size': 48,
                'lr_multiplier': 1.3,
                'optimization_level': 'HIGH_FREE'
            }
            print(f"üî• HIGH FREE MEMORY ({free_gb:.1f}GB available)")
            
        elif free_gb >= 10:  # K√∂zepes szabad mem√≥ria
            batch_config = {
                'batch_size': 32,
                'lr_multiplier': 1.1,
                'optimization_level': 'MID_HIGH_FREE'
            }
            print(f"‚ö° MID-HIGH FREE MEMORY ({free_gb:.1f}GB available)")
            
        elif free_gb >= 6:   # √Åtlagos szabad mem√≥ria
            batch_config = {
                'batch_size': 24,
                'lr_multiplier': 1.0,
                'optimization_level': 'MID_FREE'
            }
            print(f"üí™ MID FREE MEMORY ({free_gb:.1f}GB available)")
            
        elif free_gb >= 4:   # Kev√©s szabad mem√≥ria
            batch_config = {
                'batch_size': 16,
                'lr_multiplier': 0.9,
                'optimization_level': 'LOW_MID_FREE'
            }
            print(f"üéØ LOW-MID FREE MEMORY ({free_gb:.1f}GB available)")
            
        elif free_gb >= 2:   # Nagyon kev√©s szabad mem√≥ria
            batch_config = {
                'batch_size': 12,
                'lr_multiplier': 0.8,
                'optimization_level': 'LOW_FREE'
            }
            print(f"‚ö†Ô∏è LOW FREE MEMORY ({free_gb:.1f}GB available)")
            
        else:  # <2GB szabad VRAM
            print(f"‚ùå Insufficient free GPU memory (<2GB, available: {free_gb:.1f}GB)")
            print("üö® Training requires at least 2GB free VRAM.")
            print("üí° Please close other GPU applications or use a smaller model.")
            exit(1)
        
        # Mem√≥ria foglalts√°g alap√∫ finomhangol√°s
        memory_usage_ratio = allocated_memory / total_memory
        if memory_usage_ratio > 0.3:  # Ha m√°r 30%+ foglalt
            print(f"‚ö†Ô∏è High memory usage detected ({memory_usage_ratio*100:.1f}%)")
            print("üîß Reducing batch size for safety...")
            batch_config['batch_size'] = max(8, int(batch_config['batch_size'] * 0.7))
            batch_config['lr_multiplier'] *= 0.9
        
        # Safety check - dynamic memory test
        print(f"\nüß™ MEMORY SAFETY TEST")
        test_passed = True
        try:
            # Teszt tensor l√©trehoz√°sa a v√°lasztott batch size-hoz
            test_batch_size = batch_config['batch_size']
            test_tensor = torch.randn(test_batch_size, 72, device=device)
            test_tensor2 = torch.randn(test_batch_size, 64, 64, device=device)
            
            # Mem√≥ria felhaszn√°l√°s ellen≈ërz√©se
            test_memory = torch.cuda.memory_allocated(current_device)
            test_memory_gb = test_memory / (1024**3)
            
            print(f"   ‚úÖ Test batch ({test_batch_size}) allocated: {test_memory_gb:.2f} GB")
            
            # Cleanup
            del test_tensor, test_tensor2
            torch.cuda.empty_cache()
            
            # Ha a teszt t√∫l sok mem√≥ri√°t haszn√°l, cs√∂kkentj√ºk a batch size-t
            if test_memory_gb > free_gb * 0.6:  # Ha t√∂bb mint 60% szabad VRAM-ot haszn√°ln√°
                print(f"   ‚ö†Ô∏è Batch size too large for free memory, reducing...")
                batch_config['batch_size'] = max(8, int(batch_config['batch_size'] * 0.6))
                batch_config['lr_multiplier'] *= 0.9
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"   ‚ùå Memory test failed - reducing batch size")
                batch_config['batch_size'] = max(8, int(batch_config['batch_size'] * 0.5))
                batch_config['lr_multiplier'] *= 0.8
                test_passed = False
            torch.cuda.empty_cache()
        
        # V√©gs≈ë konfigur√°ci√≥
        result = {
            'batch_size': batch_config['batch_size'],
            'lr_multiplier': batch_config['lr_multiplier'],
            'memory_gb': total_gb,
            'free_memory_gb': free_gb,  # Hozz√°adva a szabad mem√≥ria info
            'device_name': gpu_name,
            'optimization_level': batch_config['optimization_level'],
            'memory_test_passed': test_passed
        }
        
        print(f"\n‚úÖ OPTIMIZED TRAINING CONFIGURATION (FREE MEMORY BASED):")
        print(f"   üéØ Batch Size: {result['batch_size']}")
        print(f"   üìà LR Multiplier: {result['lr_multiplier']:.2f}x")
        print(f"   üè∑Ô∏è Level: {result['optimization_level']}")
        print(f"   üíæ Free VRAM: {free_gb:.1f}GB / {total_gb:.1f}GB total")
        print(f"   üß™ Memory Test: {'‚úÖ PASSED' if test_passed else '‚ö†Ô∏è ADJUSTED'}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå GPU detection failed: {e}")
        print("ÔøΩ Cannot proceed without GPU information.")
        exit(1)

def load_pgn_data(pgn_path, fen_to_tensor, max_games=None, max_moves=40, min_elo=1600):
    all_states, all_policies, all_fens = [], [], []
    
    # Debug counters
    stats = {
        'total_games': 0,
        'elo_rejected': 0,
        'result_rejected': 0,
        'timecontrol_rejected': 0,
        'processed_games': 0,
        'positions_extracted': 0,
        'piece_moves': 0,
        'pawn_moves': 0,
        'captures': 0,
        'tactical_moves': 0
    }
    
    with open(pgn_path, encoding="utf-8") as pgn:
        while True:
            game = chess.pgn.read_game(pgn)
            if game is None:
                break
                
            stats['total_games'] += 1
            
            if stats['total_games'] % 1000 == 0:
                print(f"PGN: {stats['total_games']} ellen≈ërizve, {stats['processed_games']} feldolgozva, "
                      f"{stats['positions_extracted']} poz√≠ci√≥")
                      
            if max_games is not None and stats['processed_games'] >= max_games:
                break
            
            # Sz≈±r√©s: csak megfelel≈ë j√°t√©kosok j√°tszmai
            try:
                white_elo = int(game.headers.get("WhiteElo", "0"))
                black_elo = int(game.headers.get("BlackElo", "0"))
                
                # MAGASABB ELO minimum - jobb min≈ës√©g
                if not (white_elo >= min_elo and black_elo >= min_elo and 
                       white_elo < 2800 and black_elo < 2800):
                    stats['elo_rejected'] += 1
                    continue
                    
                # Id≈ëkontroll √©s eredm√©ny sz≈±r√©s
                time_control = game.headers.get("TimeControl", "")
                result = game.headers.get("Result", "*")
                
                # Csak befejezett j√°tszm√°k
                if not (result in ["1-0", "0-1"]):  # Kiz√°rjuk a d√∂ntetleneket!
                    stats['result_rejected'] += 1
                    continue
                    
                if not (len(time_control) >= 3):
                    stats['timecontrol_rejected'] += 1
                    continue
                        
                # Ha minden sz≈±r√©si felt√©tel teljes√ºl, feldolgozzuk a j√°tszm√°t
                board = game.board()
                node = game
                move_count = 0
                
                # K√ñZ√âPJ√ÅT√âK √©s V√âGJ√ÅT√âK hangs√∫lyoz√°sa
                opening_moves_to_skip = 8  # T√∂bb nyit√°s kihagy√°sa
                current_move = 0
                game_moves = []
                
                # El≈ëre gy≈±jtj√ºk a l√©p√©seket min≈ës√©gi sz≈±r√©shez
                temp_board = game.board()
                temp_node = game
                while temp_node.variations:
                    move = temp_node.variation(0).move
                    game_moves.append((temp_board.copy(), move))
                    temp_board.push(move)
                    temp_node = temp_node.variation(0)
                
                # INTELLIGENS L√âP√âS SZELEKT√ÅL√ÅS
                for board_state, move in game_moves:
                    current_move += 1
                    
                    # Skip opening moves
                    if current_move <= opening_moves_to_skip:
                        board.push(move)
                        node = node.variation(0)
                        continue
                    
                    # MIN≈êS√âGI SZ≈∞R√âS - csak √©rdekes l√©p√©sek
                    is_capture = board.is_capture(move)
                    is_check = board.gives_check(move)
                    is_piece_move = board.piece_at(move.from_square) and \
                                   board.piece_at(move.from_square).piece_type != chess.PAWN
                    is_tactical = is_capture or is_check or \
                                 board.is_castling(move) or \
                                 board.is_en_passant(move)
                    
                    # BALANCED SAMPLING - prefer√°ljuk a b√°bu l√©p√©seket √©s taktikai l√©p√©seket
                    include_move = False
                    
                    if is_tactical:  # Mindig befoglaljuk a taktikai l√©p√©seket
                        include_move = True
                        stats['tactical_moves'] += 1
                    elif is_piece_move:  # 80% es√©llyel befoglaljuk a b√°bu l√©p√©seket
                        if np.random.random() < 0.8:
                            include_move = True
                            stats['piece_moves'] += 1
                    else:  # Csak 30% es√©llyel befoglaljuk a gyalog l√©p√©seket
                        if np.random.random() < 0.3:
                            include_move = True
                            stats['pawn_moves'] += 1
                    
                    if include_move:
                        state = fen_to_tensor(board.fen())
                        policy_sparse = (move.from_square, move.to_square)
                        current_fen = board.fen()  # Store the actual FEN
                        
                        all_states.append(state)
                        all_policies.append(policy_sparse)
                        all_fens.append(current_fen)  # Store FEN for Stockfish evaluation
                        stats['positions_extracted'] += 1
                        
                        if is_capture:
                            stats['captures'] += 1
                    
                    board.push(move)
                    node = node.variation(0)
                    move_count += 1
                    
                    if move_count >= max_moves:
                        break
                
                stats['processed_games'] += 1
                        
            except (ValueError, KeyError):
                continue  # Skip games with missing/invalid data
    
    # ENHANCED statistics
    print(f"\nüìä ENHANCED PGN Processing Statistics:")
    print(f"   Total games examined: {stats['total_games']:,}")
    print(f"   Successfully processed: {stats['processed_games']:,}")
    print(f"   Positions extracted: {stats['positions_extracted']:,}")
    print(f"   ‚îú‚îÄ‚îÄ Piece moves: {stats['piece_moves']:,} ({stats['piece_moves']/max(stats['positions_extracted'], 1)*100:.1f}%)")
    print(f"   ‚îú‚îÄ‚îÄ Pawn moves: {stats['pawn_moves']:,} ({stats['pawn_moves']/max(stats['positions_extracted'], 1)*100:.1f}%)")
    print(f"   ‚îú‚îÄ‚îÄ Captures: {stats['captures']:,} ({stats['captures']/max(stats['positions_extracted'], 1)*100:.1f}%)")
    print(f"   ‚îî‚îÄ‚îÄ Tactical moves: {stats['tactical_moves']:,} ({stats['tactical_moves']/max(stats['positions_extracted'], 1)*100:.1f}%)")
    
    print(f"‚úÖ BALANCED PGN adatok: {len(all_states):,} poz√≠ci√≥ {stats['processed_games']} j√°tszm√°b√≥l")
    return all_states, all_policies, all_fens

def load_puzzle_data(csv_path, fen_to_tensor, max_puzzles=None, min_rating=800, max_rating=2200):
    """
    Lichess puzzle CSV bet√∂lt√©se taktikai training adatokhoz
    
    Args:
        csv_path: Path to lichess puzzle CSV file
        fen_to_tensor: FEN konvert√°l√≥ f√ºggv√©ny
        max_puzzles: Maximum puzzles to load
        min_rating: Minimum puzzle rating
        max_rating: Maximum puzzle rating
    
    Returns:
        puzzle_states, puzzle_policies, puzzle_fens
    """
    print(f"\nüß© LOADING TACTICAL PUZZLES from {csv_path}")
    
    puzzle_states = []
    puzzle_policies = []
    puzzle_fens = []
    
    # Puzzle statistics
    stats = {
        'total_puzzles': 0,
        'rating_filtered': 0,
        'parse_errors': 0,
        'processed_puzzles': 0,
        'difficulty_easy': 0,
        'difficulty_medium': 0,
        'difficulty_hard': 0
    }
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            
            # Skip header if present
            first_row = next(reader, None)
            if first_row and 'PuzzleId' in first_row[0]:
                pass  # Header row, continue
            else:
                # First row is data, process it
                csvfile.seek(0)
                reader = csv.reader(csvfile)
            
            for row in reader:
                if len(row) < 9:  # Ensure minimum columns
                    continue
                    
                stats['total_puzzles'] += 1
                
                if stats['total_puzzles'] % 5000 == 0:
                    print(f"Puzzles: {stats['total_puzzles']:,} ellen≈ërizve, {stats['processed_puzzles']:,} feldolgozva")
                
                if max_puzzles and stats['processed_puzzles'] >= max_puzzles:
                    break
                
                try:
                    # CSV format: PuzzleId,FEN,Moves,Rating,RatingDeviation,Popularity,NbPlays,Themes,GameUrl
                    puzzle_id = row[0]
                    fen = row[1]
                    moves = row[2].split()
                    rating = int(row[3])
                    themes = row[7] if len(row) > 7 else ""
                    
                    # Rating filter
                    if not (min_rating <= rating <= max_rating):
                        stats['rating_filtered'] += 1
                        continue
                    
                    # Parse the first move (puzzle solution)
                    if len(moves) < 1:
                        stats['parse_errors'] += 1
                        continue
                    
                    first_move = moves[0]
                    
                    # Convert UCI move to board squares
                    board = chess.Board(fen)
                    try:
                        move = chess.Move.from_uci(first_move)
                        if move not in board.legal_moves:
                            stats['parse_errors'] += 1
                            continue
                    except:
                        stats['parse_errors'] += 1
                        continue
                    
                    # Convert position to tensor
                    state = fen_to_tensor(fen)
                    policy = (move.from_square, move.to_square)
                    
                    puzzle_states.append(state)
                    puzzle_policies.append(policy)
                    puzzle_fens.append(fen)
                    stats['processed_puzzles'] += 1
                    
                    # Difficulty categorization
                    if rating < 1200:
                        stats['difficulty_easy'] += 1
                    elif rating < 1800:
                        stats['difficulty_medium'] += 1
                    else:
                        stats['difficulty_hard'] += 1
                        
                except (ValueError, IndexError, chess.InvalidMoveError) as e:
                    stats['parse_errors'] += 1
                    continue
    
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Puzzle file not found: {csv_path}")
        return [], [], []
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading puzzles: {e}")
        return [], [], []
    
    # Enhanced statistics
    print(f"\nüìä PUZZLE PROCESSING Statistics:")
    print(f"   Total puzzles examined: {stats['total_puzzles']:,}")
    print(f"   Successfully processed: {stats['processed_puzzles']:,}")
    print(f"   Rating filtered: {stats['rating_filtered']:,}")
    print(f"   Parse errors: {stats['parse_errors']:,}")
    print(f"   ‚îú‚îÄ‚îÄ Easy (< 1200): {stats['difficulty_easy']:,} ({stats['difficulty_easy']/max(stats['processed_puzzles'], 1)*100:.1f}%)")
    print(f"   ‚îú‚îÄ‚îÄ Medium (1200-1800): {stats['difficulty_medium']:,} ({stats['difficulty_medium']/max(stats['processed_puzzles'], 1)*100:.1f}%)")
    print(f"   ‚îî‚îÄ‚îÄ Hard (> 1800): {stats['difficulty_hard']:,} ({stats['difficulty_hard']/max(stats['processed_puzzles'], 1)*100:.1f}%)")
    
    print(f"‚úÖ TACTICAL PUZZLES: {len(puzzle_states):,} poz√≠ci√≥ bet√∂ltve")
    return puzzle_states, puzzle_policies, puzzle_fens

# Egyszer≈±s√≠tett board encoder - compact reprezent√°ci√≥
def fen_to_tensor(fen):
    board = chess.Board(fen)
    
    # Compact piece encoding: 4 bit el√©g lenne, de haszn√°lunk 8-bit integers
    piece_map = {
        'P': 1, 'N': 2, 'B': 3, 'R': 4, 'Q': 5, 'K': 6,
        'p': 7, 'n': 8, 'b': 9, 'r': 10, 'q': 11, 'k': 12
    }
    
    # 64 mez≈ë mint uint8 + extra inform√°ci√≥k mint float16
    board_state = np.zeros(64 + 8, dtype=np.uint8)  # Ultra compact representation
    
    # B√°buk poz√≠ci√≥i (64 dimenzi√≥ mint integer)
    for square in chess.SQUARES:
        piece = board.piece_at(square)
        if piece:
            board_state[square] = piece_map[piece.symbol()]
    
    # Extra inform√°ci√≥k (8 dimenzi√≥) - ezek maradnak float t√≠pus√∫ak
    extra_info = np.zeros(8, dtype=np.float16)
    extra_info[0] = float(board.turn)  # Ki van soron (0=fekete, 1=feh√©r)
    extra_info[1] = float(board.has_kingside_castling_rights(chess.WHITE))
    extra_info[2] = float(board.has_queenside_castling_rights(chess.WHITE))
    extra_info[3] = float(board.has_kingside_castling_rights(chess.BLACK))
    extra_info[4] = float(board.has_queenside_castling_rights(chess.BLACK))
    extra_info[5] = float(board.ep_square) if board.ep_square is not None else -1.0
    extra_info[6] = float(board.halfmove_clock) / 100.0
    extra_info[7] = float(board.fullmove_number) / 100.0
    
    # Combine board and extra info - convert to float32 for neural network
    result = np.zeros(72, dtype=np.float32)
    result[:64] = board_state[:64].astype(np.float32)  # Convert pieces to float
    result[64:] = extra_info.astype(np.float32)  # Convert extra info to float
    
    return result

def optuna_hyperparameter_optimization(states, policies, values, n_trials=20):
    """
    Optuna-based hyperparameter optimization Policy+Value modellhez
    """
    import optuna
    from optuna.samplers import TPESampler
    
    print("\nüöÄ OPTUNA HYPERPARAMETER OPTIMIZATION")
    print("="*50)
    print(f"Running {n_trials} intelligent trials with TPE sampler")
    
    dataset = PolicyValueDataset(states, policies, values)
    dataset_size = len(states)
    
    def objective(trial):
        """Optuna objective function"""
        
        # Hyperparameter suggestions
        hidden_dim = trial.suggest_categorical('hidden_dim', [128, 160, 192, 224, 256, 288])
        N = trial.suggest_int('N', 2, 10)
        T = trial.suggest_int('T', 2, 10)
        
        # Learning rate √©s batch size √ñSSZEF√úGG≈ê optimaliz√°l√°s
        # Kisebb batch ‚Üí kisebb lr (stabilabb gradiens, de lassabb konvergencia)
        # Nagyobb batch ‚Üí nagyobb lr (gyorsabb konvergencia, stabil nagy batch gradiens)
        batch_size = trial.suggest_categorical('batch_size', [16, 24, 32])
        
        # LR scaling based on batch size - empirikus batch size scaling
        lr_base = trial.suggest_float('lr_base', 8e-5, 3e-4, log=True)
        
        if batch_size == 16:
            lr = lr_base * 0.75  # 25% cs√∂kkent√©s kis batch-hez
        elif batch_size == 24:
            lr = lr_base  # Referencia lr (baseline)
        else:  # batch_size == 32
            lr = lr_base * 1.3  # 30% n√∂vel√©s nagy batch-hez (sqrt scaling k√∂zel√≠t√©s)
        
        # Model l√©trehoz√°sa
        model = HRMChess(
            input_dim=72,
            hidden_dim=hidden_dim,
            N=N,
            T=T
        ).to(device)
        
        total_params = sum(p.numel() for p in model.parameters())
        hrm_steps = N * T
        
        # Quick evaluation with early stopping
        try:
            # Mini training - csak 2 epoch gyors tesztel√©shez
            val_loss = quick_train_eval(
                model, dataset, 
                epochs=2, 
                batch_size=batch_size, 
                lr=lr,
                subset_ratio=0.1,  # Csak 10% adaton tesztel a gyorsas√°g√©rt
                device=device
            )
            
            # Penalty for too many parameters (efficiency optimization)
            param_penalty = total_params / 1_000_000  # 1M param = 1.0 penalty
            steps_penalty = max(0, hrm_steps - 10) * 0.1  # 10+ l√©p√©s penalty
            
            # Combined objective: validation loss + efficiency penalties
            objective_value = val_loss + param_penalty * 0.01 + steps_penalty * 0.02
            
            # Prune trial ha t√∫l rossz
            if val_loss > 10.0:  # Prune clearly bad trials
                raise optuna.TrialPruned()
            
            return objective_value
            
        except Exception as e:
            print(f"Trial failed: {e}")
            return float('inf')
    
    # Optuna study l√©trehoz√°sa
    sampler = TPESampler(seed=42)  # Reproducible results
    study = optuna.create_study(
        direction='minimize', 
        sampler=sampler,
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)
    )
    
    # Optimization futtat√°sa
    print(f"üî¨ Starting {n_trials} Optuna trials...")
    study.optimize(objective, n_trials=n_trials, timeout=1800)  # Max 30 min
    
    # Legjobb eredm√©nyek
    print(f"\nüèÜ OPTUNA OPTIMIZATION RESULTS")
    print("="*50)
    
    best_trial = study.best_trial
    best_params = best_trial.params
    
    print(f"Best trial value: {best_trial.value:.4f}")
    print(f"Best parameters:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    
    # Top 5 trial eredm√©nyek
    print(f"\nüìä Top 5 trials:")
    sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))
    for i, trial in enumerate(sorted_trials[:5]):
        if trial.value is not None:
            print(f"{i+1}. Value: {trial.value:.4f}, Params: {trial.params}")
    
    # Visszat√©r√©s optim√°lis konfigur√°ci√≥val
    # Helyes lr kisz√°m√≠t√°sa lr_base √©s batch_size alapj√°n
    lr_base = best_params['lr_base']
    batch_size = best_params['batch_size']
    
    if batch_size == 16:
        lr = lr_base * 0.75
    elif batch_size == 24:
        lr = lr_base
    else:  # batch_size == 32
        lr = lr_base * 1.3
    
    best_config = {
        'hidden_dim': best_params['hidden_dim'],
        'N': best_params['N'],
        'T': best_params['T'],
        'lr': lr,  # Sz√°m√≠tott lr √©rt√©k
        'lr_base': lr_base,  # Eredeti lr_base is meg≈ërz√©se
        'batch_size': batch_size,
        'name': f"Optuna-Optimized-{best_params['N']}x{best_params['T']}-BS{batch_size}"
    }
    
    return best_config

def get_user_choice():
    """Interactive parameter selection"""
    print("\n" + "="*60)
    print("ÔøΩ HRM CHESS MODEL CONFIGURATION")
    print("="*60)
    print("Choose your training approach:")
    print("1. üîß Hyperparameter Optimization (Automatic - finds best N, T, hidden_dim)")
    print("2. ‚öôÔ∏è  Manual Parameter Settings (Choose your own N, T, hidden_dim)")
    
    while True:
        try:
            choice = input("\nEnter your choice (1-2): ").strip()
            if choice in ['1', '2']:
                return int(choice)
            else:
                print("‚ùå Please enter 1 or 2")
        except (ValueError, KeyboardInterrupt):
            print("\n‚ùå Invalid input. Please enter 1 or 2")

def get_manual_parameters():
    """Get manual hyperparameters from user"""
    print("\nüîß MANUAL PARAMETER CONFIGURATION")
    print("-" * 40)
    print("üí° Parameter Guidelines:")
    print("   ‚Ä¢ hidden_dim: 128-512 (network width)")
    print("   ‚Ä¢ N: 2-8 (high-level reasoning cycles)")
    print("   ‚Ä¢ T: 2-8 (steps per cycle)")
    print("   ‚Ä¢ Total HRM steps = N √ó T (recommended: 6-32)")
    print("   ‚Ä¢ More steps = better reasoning but slower training")
    
    # Get hidden_dim
    while True:
        try:
            hidden_dim = int(input("\nEnter hidden_dim (128-512, recommended 192-256): "))
            if 64 <= hidden_dim <= 1024:
                break
            else:
                print("‚ùå Please enter a value between 64 and 1024")
        except ValueError:
            print("‚ùå Please enter a valid integer")
    
    # Get N
    while True:
        try:
            N = int(input("Enter N - reasoning cycles (2-12, recommended 2-12): "))
            if 2 <= N <= 12:
                break
            else:
                print("‚ùå Please enter a value between 2 and 12")
        except ValueError:
            print("‚ùå Please enter a valid integer")
    
    # Get T
    while True:
        try:
            T = int(input("Enter T - steps per cycle (2-12, recommended 3-12): "))
            if 2 <= T <= 12:
                break
            else:
                print("‚ùå Please enter a value between 2 and 12")
        except ValueError:
            print("‚ùå Please enter a valid integer")
    
    total_steps = N * T
    
    # Estimate model complexity
    estimated_params = hidden_dim * (hidden_dim * 6 + 64*64) + hidden_dim * 3
    complexity_level = "Light" if total_steps <= 8 else "Medium" if total_steps <= 16 else "Heavy"
    
    print(f"\n‚úÖ Manual Configuration:")
    print(f"   ‚Ä¢ hidden_dim: {hidden_dim}")
    print(f"   ‚Ä¢ N: {N}, T: {T}")
    print(f"   ‚Ä¢ Total HRM steps: {total_steps}")
    print(f"   ‚Ä¢ Complexity: {complexity_level}")
    print(f"   ‚Ä¢ Estimated parameters: ~{estimated_params:,}")
    
    if total_steps > 50:
        print("‚ö†Ô∏è  Warning: Very high step count may slow training significantly")
    elif total_steps > 32:
        print("‚ö†Ô∏è  Warning: High step count may slow training")
    elif total_steps < 4:
        print("‚ö†Ô∏è  Warning: Very low step count may reduce model capacity")
    
    # Confirmation
    confirm = input("\nProceed with this configuration? (y/n): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("üîÑ Restarting parameter selection...")
        return get_manual_parameters()
    
    return hidden_dim, N, T

class StockfishEvaluator:
    """Stockfish motor integr√°ci√≥ poz√≠ci√≥ √©rt√©kel√©shez - PERZISZTENS KAPCSOLAT"""
    
    def __init__(self, stockfish_path="./stockfish.exe", movetime=50):
        self.stockfish_path = stockfish_path
        self.movetime = movetime
        self.process = None
        self.initialized = False
        self._init_engine()
    
    def _init_engine(self):
        """Stockfish motor inicializ√°l√°s - H√ÅTT√âR PROCESS"""
        try:
            if os.path.exists(self.stockfish_path):
                print(f"ü§ñ Stockfish found: {self.stockfish_path}")
            else:
                print(f"‚ùå Stockfish not found at: {self.stockfish_path}")
                print("üîç Looking for stockfish in system PATH...")
                # Try system stockfish
                self.stockfish_path = "stockfish"
            
            # Start persistent Stockfish process
            print("üöÄ Starting persistent Stockfish engine...")
            self.process = subprocess.Popen(
                [self.stockfish_path],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,  # Line buffered
                universal_newlines=True
            )
            
            # Initialize UCI
            self._send_command("uci")
            self._wait_for_response("uciok")
            
            self._send_command("isready")
            self._wait_for_response("readyok")
            
            print("‚úÖ Persistent Stockfish engine ready!")
            self.initialized = True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Stockfish initialization error: {e}")
            self.initialized = False
    
    def _send_command(self, command):
        """Send command to Stockfish"""
        if self.process and self.process.stdin:
            try:
                self.process.stdin.write(command + "\n")
                self.process.stdin.flush()
            except:
                self.initialized = False
    
    def _read_line(self, timeout=2.0):
        """Read a line from Stockfish with timeout - Windows kompatibilis"""
        if not self.process or not self.process.stdout:
            return None
        
        try:
            # Windows-friendly approach - poll and readline
            import time
            start_time = time.time()
            
            while time.time() - start_time < timeout:
                # Check if process is still alive
                if self.process.poll() is not None:
                    return None
                
                # Try to read a line (non-blocking simulation)
                try:
                    # This will block, but we have a short timeout
                    line = self.process.stdout.readline()
                    if line:
                        return line.strip()
                except:
                    break
                
                # Small sleep to prevent busy waiting
                time.sleep(0.01)
            
        except Exception as e:
            return None
        
        return None
    
    def _wait_for_response(self, expected, timeout=5.0):
        """Wait for specific response from Stockfish"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            line = self._read_line(0.1)
            if line and expected in line:
                return True
        return False
    
    def evaluate_position(self, fen):
        """
        Poz√≠ci√≥ √©rt√©kel√©se Stockfish-sel
        Returns: (best_move_uci, evaluation_score)
        """
        if not self.initialized or not self.process:
            return None, 0.0
        
        try:
            # Set position
            self._send_command(f"position fen {fen}")
            
            # Get evaluation with limited thinking time
            self._send_command(f"go movetime {self.movetime}")
            
            # Parse response - simplified approach for Windows
            best_move = None
            score = 0.0
            lines_read = 0
            max_lines = 50  # Limit reading to prevent hanging
            
            while lines_read < max_lines:
                try:
                    line = self.process.stdout.readline()
                    if not line:
                        break
                    
                    line = line.strip()
                    lines_read += 1
                    
                    if line.startswith('bestmove'):
                        parts = line.split()
                        if len(parts) >= 2 and parts[1] != "(none)":
                            best_move = parts[1]
                        break
                    elif 'score cp' in line:
                        # Extract centipawn score
                        try:
                            parts = line.split()
                            for i, part in enumerate(parts):
                                if part == "cp" and i + 1 < len(parts):
                                    cp_score = int(parts[i + 1])
                                    score = max(-1.0, min(1.0, cp_score / 300.0))
                                    break
                        except:
                            pass
                    elif 'score mate' in line:
                        # Mate score
                        try:
                            parts = line.split()
                            for i, part in enumerate(parts):
                                if part == "mate" and i + 1 < len(parts):
                                    mate_moves = int(parts[i + 1])
                                    score = 1.0 if mate_moves > 0 else -1.0
                                    break
                        except:
                            pass
                            
                except Exception as e:
                    break
            
            return best_move, score
            
        except Exception as e:
            print(f"‚ö†Ô∏è Stockfish evaluation error: {e}")
            return None, 0.0
    
    def close(self):
        """Close Stockfish engine"""
        if self.process:
            try:
                self._send_command("quit")
                self.process.terminate()
                self.process.wait(timeout=2)
            except:
                if self.process:
                    self.process.kill()
            self.process = None
            self.initialized = False
            print("üîå Stockfish engine closed")
    
    def __del__(self):
        """Cleanup when object is destroyed"""
        self.close()
    
    def create_policy_value_dataset(self, states, policies, max_positions=10000):
        """
        Policy+Value dataset l√©trehoz√°sa Stockfish √©rt√©kel√©ssel
        """
        print(f"\nü§ñ Creating Policy+Value dataset with Stockfish...")
        print(f"üìä Processing {min(len(states), max_positions):,} positions")
        
        # Limit positions for reasonable processing time
        total_positions = min(len(states), max_positions)
        
        stockfish_states = []
        stockfish_policies = []
        stockfish_values = []
        
        processed = 0
        start_time = time.time()
        
        for i in range(total_positions):
            if i % 1000 == 0:
                elapsed = time.time() - start_time
                rate = i / elapsed if elapsed > 0 else 0
                eta = (total_positions - i) / rate if rate > 0 else 0
                print(f"üìà Progress: {i:,}/{total_positions:,} ({i/total_positions*100:.1f}%) "
                      f"Rate: {rate:.1f}/s ETA: {eta/60:.1f}min")
            
            try:
                # Convert state back to FEN for Stockfish
                state = states[i]
                policy = policies[i]
                
                # Simple state to FEN conversion (simplified)
                board = chess.Board()
                # Note: This is a simplified conversion, a proper implementation
                # would reconstruct the exact position from the state vector
                
                # For now, use a basic starting position and get some evaluation
                fen = board.fen()
                
                # Get Stockfish evaluation
                best_move, value = self.evaluate_position(fen)
                
                if best_move is not None:
                    stockfish_states.append(state)
                    stockfish_policies.append(policy)
                    stockfish_values.append(value)
                    processed += 1
                
            except Exception as e:
                continue  # Skip problematic positions
        
        print(f"\n‚úÖ Created Policy+Value dataset:")
        print(f"   üìä Processed: {processed:,} positions")
        print(f"   üìà Success rate: {processed/total_positions*100:.1f}%")
        
        return np.array(stockfish_states), stockfish_policies, np.array(stockfish_values)

def create_policy_value_dataset_from_games(max_positions=10000):
    """
    Policy+Value dataset l√©trehoz√°sa j√°tszm√°kb√≥l √©s Stockfish √©rt√©kel√©ssel
    
    Args:
        max_positions: Maxim√°lis poz√≠ci√≥k sz√°ma a dataset-ben
    """
    print("\nüéÆ Creating Policy+Value dataset from games...")
    print(f"üéØ Target positions: {max_positions:,}")
    
    # Intelligens PGN games becsl√©s a dataset m√©ret alapj√°n
    avg_positions_per_game = 22  # √Åtlagos poz√≠ci√≥k sz√°ma j√°tszm√°nk√©nt
    
    # PGN target sz√°m√≠t√°s - √°ltal√°nos sk√°l√°zhat√≥ m√≥dszer
    # Logaritmikus sk√°l√°z√°s minden dataset m√©retre
    
    # Logaritmikus sk√°l√°z√°s: 50% puzzle kis dataset-n√©l ‚Üí 15% nagy dataset-n√©l
    log_factor = math.log10(max(max_positions, 1000))
    base_ratio = 0.5  # 50% kezdeti puzzle ratio
    scale_factor = (log_factor - 3.0) / 4.0  # 3=log10(1000), 7=log10(10M)
    scale_factor = max(0, min(1, scale_factor))  # Clamp 0-1 k√∂z√©
    
    # Puzzle ratio: 50% (1k) ‚Üí 35% (100k) ‚Üí 20% (10M) ‚Üí 15% (100M+)
    puzzle_ratio = base_ratio - (scale_factor * 0.35)
    
    # PGN ratio - komplementer
    pgn_ratio = 1.0 - puzzle_ratio
    pgn_target_positions = int(max_positions * pgn_ratio)
    estimated_games_needed = max(1000, int(pgn_target_positions / avg_positions_per_game))
    
    print(f"üéØ Automatic scaling for {max_positions:,} positions:")
    print(f"   üìä PGN ratio: {pgn_ratio:.1%} ‚Üí {pgn_target_positions:,} positions")  
    print(f"   üß© Puzzle ratio: {puzzle_ratio:.1%} ‚Üí {int(max_positions * puzzle_ratio):,} positions")
    print(f"   üéÆ Estimated games needed: {estimated_games_needed:,}")
    
    # Load PGN data
    print(f"üì• Loading PGN games...")
    try:
        pgn_states, pgn_policies, pgn_fens = load_pgn_data(
            "./lichess_db_standard_rated_2013-07.pgn",
            fen_to_tensor,
            max_games=estimated_games_needed,
            max_moves=30,
            min_elo=1600
        )
        print(f"‚úÖ Loaded {len(pgn_states):,} positions from PGN")
    except:
        print("‚ö†Ô∏è PGN file not found, using minimal dataset")
        pgn_states, pgn_policies, pgn_fens = [], [], []
    
    # Load PUZZLE data - TAKTIKAI K√âPESS√âGEK - SK√ÅL√ÅZHAT√ì M√âRETEZ√âS
    print(f"üì• Loading tactical puzzles...")
    
    # √Åltal√°nos puzzle ratio sz√°m√≠t√°s - logaritmikus sk√°l√°z√°s
    # Kisebb dataset-ekn√©l t√∂bb puzzle (jobb taktikai f√≥kusz)
    # Nagyobb dataset-ekn√©l kevesebb puzzle (PGN dominancia)
    
    # Logaritmikus sk√°l√°z√°s: 50% puzzle kis dataset-n√©l ‚Üí 15% nagy dataset-n√©l
    log_factor = math.log10(max(max_positions, 1000))  # min 1000 a log hib√°k elker√ºl√©s√©re
    base_ratio = 0.5  # 50% kezdeti puzzle ratio
    scale_factor = (log_factor - 3.0) / 4.0  # 3=log10(1000), 7=log10(10M) ‚Üí 0-1 range
    scale_factor = max(0, min(1, scale_factor))  # Clamp 0-1 k√∂z√©
    
    # Puzzle ratio: 50% (1k) ‚Üí 35% (100k) ‚Üí 20% (10M) ‚Üí 15% (100M+)
    puzzle_ratio = base_ratio - (scale_factor * 0.35)
    puzzle_target = int(max_positions * puzzle_ratio)
    
    print(f"üéØ Dataset size: {max_positions:,} ‚Üí Puzzle ratio: {puzzle_ratio:.1%}")
    print(f"üéØ Target puzzle count: {puzzle_target:,}")
    
    # PGN ratio kalkul√°ci√≥
    pgn_ratio = 1.0 - puzzle_ratio
    pgn_target_positions = int(max_positions * pgn_ratio)
    
    try:
        puzzle_states, puzzle_policies, puzzle_fens = load_puzzle_data(
            "./lichess_db_puzzle.csv",
            fen_to_tensor,
            max_puzzles=puzzle_target,
            min_rating=800,   # Alacsonyabb minimum t√∂bb puzzle-√©rt
            max_rating=2500   # Magasabb maximum t√∂bb puzzle-√©rt
        )
        print(f"‚úÖ Loaded {len(puzzle_states):,} tactical positions from CSV")
    except:
        print("‚ö†Ô∏è Puzzle CSV file not found, using only PGN data")
        puzzle_states, puzzle_policies, puzzle_fens = [], [], []
    
    # BALANCED DATASET COMBINATION
    print(f"\nüéØ COMBINING PGN + PUZZLE DATA:")
    print(f"   üìä PGN positions: {len(pgn_states):,}")
    print(f"   üß© Puzzle positions: {len(puzzle_states):,}")
    
    # Dataset composition analysis
    total_loaded = len(pgn_states) + len(puzzle_states)
    if total_loaded > 0:
        pgn_percentage = len(pgn_states) / total_loaded * 100
        puzzle_percentage = len(puzzle_states) / total_loaded * 100
        print(f"   üìà Composition: {pgn_percentage:.1f}% PGN, {puzzle_percentage:.1f}% Puzzles")
        
        if len(puzzle_states) < puzzle_target * 0.5:
            print(f"   ‚ö†Ô∏è Warning: Only {len(puzzle_states):,} puzzles loaded (target: {puzzle_target:,})")
            print(f"   üí° Consider checking puzzle CSV file or lowering rating filters")
    
    # Combine all data sources
    all_states = pgn_states + puzzle_states
    all_policies = pgn_policies + puzzle_policies
    all_fens = pgn_fens + puzzle_fens
    
    if len(all_states) == 0:
        print("‚ùå No training data available!")
        return None, None, None
    
    print(f"üìä Total positions loaded: {len(all_states):,}")
    
    # Dataset adequacy check
    adequacy_ratio = len(all_states) / max_positions if max_positions > 0 else 0
    if adequacy_ratio < 0.5:
        print(f"‚ö†Ô∏è Warning: Only {adequacy_ratio*100:.1f}% of target positions loaded!")
        print(f"üí° Consider: checking file paths, lowering rating filters, or reducing target size")
    elif adequacy_ratio >= 1.0:
        print(f"‚úÖ Excellent: {adequacy_ratio:.1f}x target positions available")
    else:
        print(f"‚úÖ Good: {adequacy_ratio*100:.1f}% of target positions loaded")
    
    # Adjust to target max_positions
    if len(all_states) > max_positions:
        print(f"üéØ Randomly selecting {max_positions:,} positions from {len(all_states):,} available")
        indices = np.random.choice(len(all_states), max_positions, replace=False)
        all_states = [all_states[i] for i in indices]
        all_policies = [all_policies[i] for i in indices]
        all_fens = [all_fens[i] for i in indices]
    
    # Create Stockfish evaluator for real position evaluation
    evaluator = StockfishEvaluator()
    
    # GYORS STOCKFISH BATCH EVALUATION
    print("ü§ñ Fast batch evaluation with persistent Stockfish...")
    
    values = []
    processed_states = []
    processed_policies = []
    
    # Batch processing with progress tracking
    batch_size = 1000  # Process in batches for better progress tracking
    total_batches = (len(all_states) + batch_size - 1) // batch_size
    
    start_time = time.time()
    processed_count = 0
    
    for batch_idx in range(total_batches):
        batch_start = batch_idx * batch_size
        batch_end = min(batch_start + batch_size, len(all_states))
        
        # Progress update
        elapsed = time.time() - start_time
        rate = processed_count / elapsed if elapsed > 0 else 0
        eta = (len(all_states) - processed_count) / rate if rate > 0 else 0
        
        print(f"üìà Batch {batch_idx+1}/{total_batches} | "
              f"Progress: {processed_count:,}/{len(all_states):,} ({processed_count/len(all_states)*100:.1f}%) | "
              f"Rate: {rate:.1f}/s | ETA: {eta/60:.1f}min")
        
        # Process batch
        for i in range(batch_start, batch_end):
            state = all_states[i]
            policy = all_policies[i]
            fen = all_fens[i]
            
            try:
                # Use persistent Stockfish connection - MUCH FASTER
                best_move, stockfish_value = evaluator.evaluate_position(fen)
                
                if best_move is not None:
                    value = stockfish_value
                else:
                    # Fallback to neutral if Stockfish fails
                    value = 0.0
                    
            except Exception as e:
                # Fallback value if evaluation fails
                value = 0.0
            
            values.append(value)
            processed_states.append(state)
            processed_policies.append(policy)
            processed_count += 1
    
    # Close Stockfish engine
    evaluator.close()
    
    values = np.array(values)
    
    print(f"\n‚úÖ Policy+Value dataset created:")
    print(f"   üìä Positions: {len(processed_states):,}")
    print(f"   üìà Value range: [{np.min(values):.3f}, {np.max(values):.3f}]")
    print(f"   üìä Value mean: {np.mean(values):.3f} ¬± {np.std(values):.3f}")
    
    return processed_states, processed_policies, values

if __name__ == "__main__":
    print("üèóÔ∏è HRM CHESS MODEL TRAINING")
    print("="*50)
    
    import os
    import sys
    print(f"Using device: {device}")
    
    # GPU MEMORY DETECTION & OPTIMIZATION
    gpu_config = detect_gpu_memory_and_optimize_training()
    
    # Load or create Policy+Value dataset
    pv_dataset_path = "chess_policy_value_dataset.pt"
    
    if not os.path.exists(pv_dataset_path):
        print(f"\nüìù Policy+Value dataset not found: {pv_dataset_path}")
        print("ÔøΩ Creating new Policy+Value dataset...")
        
        # Ask user for dataset size
        while True:
            try:
                max_positions = int(input("Enter dataset size (number of positions, e.g., 20000): "))
                if max_positions > 0:
                    break
                else:
                    print("‚ùå Please enter a positive number")
            except ValueError:
                print("‚ùå Please enter a valid integer")
        
        print(f"üéØ Creating dataset with {max_positions:,} positions")
        
        # Create dataset from games and puzzles with user-specified size
        states, policies, values = create_policy_value_dataset_from_games(
            max_positions=max_positions   # User-specified dataset size
        )
        
        if states is None:
            print("‚ùå Failed to create dataset!")
            exit(1)
        
        # Save dataset
        print("üíæ Saving Policy+Value dataset...")
        dataset_info = {
            'states': np.array(states, dtype=np.float32),
            'policies': policies,
            'values': values,
            'info': {
                'created': time.time(),
                'source': 'PGN + Tactical Puzzles (Stockfish evaluation)',
                'positions': len(states),
                'stockfish_depth': 'simplified',
                'data_mix': 'Balanced PGN games + tactical puzzles',
                'gpu_optimized': True,
                'gpu_config': gpu_config,
                'user_specified_size': max_positions
            }
        }
        
        torch.save(dataset_info, pv_dataset_path)
        print(f"‚úÖ Dataset saved to: {pv_dataset_path}")
        
        # Use the created data
        data = dataset_info
    else:
        # Load existing Policy+Value dataset
        print(f"\nüì• Loading existing Policy+Value dataset: {pv_dataset_path}")
        data = torch.load(pv_dataset_path, weights_only=False)
    
    states = data['states']
    policies = data['policies']
    values = data['values']
    info = data['info']
    
    print(f"‚úÖ Loaded Policy+Value dataset:")
    print(f"   üìä Positions: {len(states):,}")
    print(f"   üìà Value range: [{np.min(values):.3f}, {np.max(values):.3f}]")
    print(f"   üìä Value mean: {np.mean(values):.3f} ¬± {np.std(values):.3f}")
    print(f"   ü§ñ Source: {info.get('source', 'Unknown')}")
    print(f"   üéÆ Data mix: {info.get('data_mix', 'Unknown composition')}")
    print(f"   üñ•Ô∏è GPU Optimized: {info.get('gpu_optimized', False)}")
    
    # Policy+Value training mode
    print("\nüéØ POLICY+VALUE TRAINING MODE")
    print("   ‚Ä¢ Input: 72-dim vector ‚Üí 8x8 2D conv + extra features")
    print("   ‚Ä¢ Policy Head: 64x64 move matrix")
    print("   ‚Ä¢ Value Head: Position evaluation [-1, 1]")
    print("   ‚Ä¢ Training: Stockfish supervision")
    print("   ‚Ä¢ Dataset: Balanced PGN games + tactical puzzles")
    
    print("\n‚öôÔ∏è HRM PARAMETER EXPLANATION:")
    print("   ‚Ä¢ N: Number of high-level reasoning cycles")
    print("   ‚Ä¢ T: Steps per cycle (low-level processing)")
    print("   ‚Ä¢ Total HRM steps = N √ó T")
    print("   ‚Ä¢ hidden_dim: Neural network width")
    print("   ‚Ä¢ Optimal balance: complexity vs speed vs accuracy")
    
    # Configuration
    data_path = pv_dataset_path
    model_path = "hrm_policy_value_chess_model.pt"
    
    # Get dataset info
    dataset_size = len(states)
    print(f"\nüìä Dataset size: {dataset_size:,} positions")
    
    # Interactive parameter selection
    user_choice = get_user_choice()
    
    if user_choice == 1:
        # HYPERPARAMETER OPTIMIZATION MODE
        print("\nüî¨ HYPERPARAMETER OPTIMIZATION MODE ENABLED")
        
        # Optuna hyperparameter optimization
        try:
            import optuna
            print("‚úÖ Optuna available - using advanced Bayesian optimization")
            best_config = optuna_hyperparameter_optimization(states, policies, values, n_trials=15)
        except ImportError:
            print("‚ùå Optuna not installed - please install with: pip install optuna")
            print("ÔøΩ Falling back to manual configuration...")
            hidden_dim, N, T = get_manual_parameters()
            lr = 2e-4
            batch_size = 32
            best_config = None
        
        if best_config:
            hidden_dim = best_config['hidden_dim']
            N = best_config['N']
            T = best_config['T']
            
            # GPU-optimized batch size √©s learning rate
            gpu_batch_size = gpu_config['batch_size']
            gpu_lr_multiplier = gpu_config['lr_multiplier']
            
            # SMART GPU SCALING: Use Optuna as baseline, scale up with GPU power
            if 'batch_size' in best_config:
                optuna_batch = best_config['batch_size']
                # GPU scaling: if GPU can handle more, use more!
                batch_size = max(optuna_batch, min(gpu_batch_size, optuna_batch * 2))  # Cap at 2x Optuna or GPU limit
                print(f"üîß Batch size: Optuna suggested {optuna_batch}, GPU scaled to {batch_size} (limit: {gpu_batch_size})")
            else:
                batch_size = gpu_batch_size
                
            # Adjust learning rate for final batch size (not just GPU multiplier)
            if 'lr' in best_config:
                base_lr = best_config['lr']
            else:
                base_lr = 2e-4
                
            # Scale LR for both GPU power and actual batch size used
            batch_scaling = batch_size / best_config.get('batch_size', 16)  # Scale from Optuna baseline
            lr = base_lr * gpu_lr_multiplier * batch_scaling  # Both GPU and batch scaling
                
            model_size = f"GPU_OPTIMIZED-{best_config['name']}"
            print(f"\n‚úÖ Using GPU-optimized configuration:")
            print(f"   üéØ Final LR: {lr:.6f} (base: {base_lr:.6f} √ó GPU: {gpu_lr_multiplier:.2f} √ó batch: {batch_scaling:.2f})")
            print(f"   üìä Final Batch: {batch_size} (GPU-enhanced from {best_config.get('batch_size', 16)})")
            print(f"   üñ•Ô∏è GPU Level: {gpu_config['optimization_level']}")
            print(f"   üñ•Ô∏è VRAM: {gpu_config['memory_gb']:.1f} GB ({gpu_config['device_name']})")
        else:
            print("‚ö†Ô∏è Hyperopt failed, using GPU-optimized defaults")
            hidden_dim, N, T = 192, 3, 4
            batch_size = gpu_config['batch_size']
            lr = 2e-4 * gpu_config['lr_multiplier']
            model_size = f"GPU_DEFAULT-{gpu_config['optimization_level']}"
            
    elif user_choice == 2:
        # MANUAL PARAMETER MODE with GPU optimization
        hidden_dim, N, T = get_manual_parameters()
        
        # Apply GPU optimizations
        batch_size = gpu_config['batch_size']
        lr = 2e-4 * gpu_config['lr_multiplier']
        model_size = f"GPU_MANUAL-{N}x{T}-{gpu_config['optimization_level']}"
        
        print(f"\nüîß GPU OPTIMIZATIONS APPLIED:")
        print(f"   üìä Batch Size: {batch_size} (GPU-optimized)")
        print(f"   üìà Learning Rate: {lr:.6f} (base: 2e-4 √ó {gpu_config['lr_multiplier']:.2f})")
        print(f"   üñ•Ô∏è GPU Level: {gpu_config['optimization_level']}")
    
    # HRM modell l√©trehoz√°sa optimaliz√°lt param√©terekkel
    model = HRMChess(input_dim=72, hidden_dim=hidden_dim, N=N, T=T).to(device)
    
    # Model info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    hrm_steps = N * T
    
    print(f"\nüèóÔ∏è MODEL ARCHITECTURE:")
    print(f"üìä Total parameters: {total_params:,}")
    print(f"üìä Trainable parameters: {trainable_params:,}")
    print(f"üîÑ HRM reasoning steps: {hrm_steps} (N={N} √ó T={T})")
    print("üèóÔ∏è Architecture: HRM")
    print("   ‚Ä¢ Board conv: 8x8 ‚Üí conv2d ‚Üí 4x4 ‚Üí flatten")
    print("   ‚Ä¢ Extra processor: 8-dim meta info ‚Üí linear")
    print("   ‚Ä¢ Feature combiner: board + extra ‚Üí hidden_dim")
    print("   ‚Ä¢ Board enhancer: hidden_dim ‚Üí hidden_dim ‚Üí hidden_dim")
    print(f"   ‚Ä¢ HRM modules: L_net and H_net with N={N}, T={T}")
    print("   ‚Ä¢ Policy Head: Move prediction (hidden_dim ‚Üí 64*64)")
    print("   ‚Ä¢ Value Head: Position evaluation (hidden_dim ‚Üí 1)")
    
    # GPU-optimized training configuration
    epochs = 30  # T√∂bb epoch a jobb konvergenci√°√©rt
    
    print(f"\n‚öôÔ∏è GPU-OPTIMIZED TRAINING CONFIGURATION:")
    print(f"   ‚Ä¢ Model: {model_size}")
    print(f"   ‚Ä¢ Mode: Policy+Value+Warmup")
    print(f"   ‚Ä¢ Batch size: {batch_size} (GPU-optimized)")
    print(f"   ‚Ä¢ Learning rate: {lr:.6f} (GPU-scaled)")
    print(f"   ‚Ä¢ Warmup epochs: 3 (linear warmup + cosine annealing)")
    print(f"   ‚Ä¢ Total epochs: {epochs}")
    print(f"   ‚Ä¢ HRM steps: {N}√ó{T}={N*T}")
    print(f"   ‚Ä¢ Parameters: {total_params:,}")
    print(f"   ‚Ä¢ Dataset: {dataset_size:,} positions")
    print(f"   üñ•Ô∏è GPU: {gpu_config['device_name']} ({gpu_config['memory_gb']:.1f} GB)")
    print(f"   üè∑Ô∏è Optimization Level: {gpu_config['optimization_level']}")
    
    # Memory usage estimation
    estimated_memory_per_batch = (batch_size * 72 * 4 + batch_size * 64 * 64 * 4) / (1024**3)  # Rough estimate in GB
    print(f"   üìä Estimated memory/batch: ~{estimated_memory_per_batch:.2f} GB")
    
    # Create Policy+Value dataset
    dataset = PolicyValueDataset(states, policies, values)
    print(f"\nüìä Policy+Value dataset: {len(dataset):,} positions")
    print("üöÄ Starting GPU-optimized Policy+Value HRM training with warmup...")
    
    # Train with GPU-optimized parameters and warmup
    train_loop(model, dataset, epochs=epochs, batch_size=batch_size, lr=lr, warmup_epochs=3, device=device)
    
    # Save final model with hyperparameters and GPU info
    final_checkpoint = {
        'model_state_dict': model.state_dict(),
        'hyperparams': {
            'hidden_dim': hidden_dim,
            'N': N,
            'T': T,
            'input_dim': 72
        },
        'training_info': {
            'epochs': epochs,
            'batch_size': batch_size,
            'lr': lr,
            'warmup_epochs': 3,
            'dataset_size': len(dataset),
            'total_params': total_params,
            'training_mode': 'policy_value_warmup',
            'gpu_optimized': True,
            'gpu_config': gpu_config
        }
    }
    torch.save(final_checkpoint, model_path)
    print("\n‚úÖ Training completed!")
    print(f"üíæ Model saved to: {model_path}")
    print(f"üèÜ HRM (N={N}, T={T}, hidden_dim={hidden_dim}) with {total_params:,} parameters")
    print(f"üìä Trained on {len(dataset):,} positions with Policy+Value+Warmup mode")
    print(f"üéÆ Dataset: Balanced PGN games + tactical puzzles for enhanced gameplay")
    print(f"üî• Warmup: 3 epochs with linear warmup + cosine annealing")
    
    print("üéØ Expected: Enhanced move prediction + position evaluation + tactical strength")
    print("‚öîÔ∏è Ready for tactical fine-tuning and stronger gameplay!")

